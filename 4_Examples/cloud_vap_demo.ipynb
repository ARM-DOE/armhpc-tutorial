{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ARM Live credentials here\n",
    "user = \"pricezt\"\n",
    "credential = \"65e64ac512d85f23\"\n",
    "\n",
    "# Get your credentials at https://adc.arm.gov/armlive/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import shutil\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data from the ARM archive\n",
    "Thr ARMLive web service returns a JSON blob with download links for archive files based on the datastream, start, and end dates provided. This function parses the JSON blob and downloads the responsive files into the output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_arm_files(user, token, datastream, start, end, output_directory):\n",
    "    params = {\n",
    "       'user': f'{user}:{token}',\n",
    "       'ds': datastream,\n",
    "       'start': start,\n",
    "       'end': end,\n",
    "       'wt': 'json',\n",
    "    }\n",
    "\n",
    "#     print(params)\n",
    "    response = requests.get('https://adc.arm.gov/armlive/livedata/query', params=params)\n",
    "#     print(response.url)\n",
    "    response = response.json()\n",
    "#     print(response)\n",
    "    downloaded_files = []\n",
    "    for filename in response['files']:\n",
    "        download_url = f'https://adc.arm.gov/armlive/livedata/saveData?user={user}:{token}&file={filename}'\n",
    "        file_path = Path(output_directory, filename)\n",
    "        file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with requests.get(download_url, stream=True) as r:\n",
    "            with open(file_path, 'wb') as f:\n",
    "                shutil.copyfileobj(r.raw, f)\n",
    "        downloaded_files.append(file_path)\n",
    "    return downloaded_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Note for users of Stratus:\n",
    "The above function uses the ARMLive web service which is available from anywhere in the world, no setup needed. The drawback to this approach is that the service only provides \"user accessible\" data, which excludes raw and some other types of data. Within the Stratus HPC environment, we provide an HPC module and python library which can be used to stage any file in the archive.\n",
    "\n",
    "**PLEASE NOTE**: while we plan to have these options available within the Jupyter environments and outside of the ADC shortly, currently **`stage_arm_data` is only avaliable from the login and compute nodes of Stratus**. You may use these commands interactively or within PBS submission scripts, but not from Jupyter notebook code cells (yet). The recommended workaround in the meantime is to open a web terminal in Jupyter, ssh to `stratus.ornl.gov`, and stage data so that you can access it from within the Jupyter environment.\n",
    "\n",
    "You can access these functions by running `module load stage_arm_data` from the terminal. After loading the module, the `stage_arm_data` command will be added to your `PATH` and the `stage_arm_data` python library will be added to your `PYTHONPATH`. An example invocation from the command line would look like this:\n",
    "\n",
    "```bash\n",
    "stage_arm_data --to Stratus --datastream corkasacrcfrhsrhiM1.a1 --start 2019-01-01T00:00:00 --end 2019-02-01T00:00:00\n",
    "```\n",
    "This will stage all ARM archive files for the `corkasacrcfrhsrhiM1.a1` from the `2019-01-01T00:00:00` til `2019-02-01T00:00:00` time period to the staging directory at `/lustre/or-hydra/cades-arm/proj-shared/data_transfer/cor/corkasacrcfrhsrhiM1.a1/`.\n",
    "\n",
    "From within a python script, you can also use the provided library to accomplish the same thing:\n",
    "\n",
    "```python\n",
    "from stage_arm_data.core import transfer_files\n",
    "from stage_arm_data.endpoints import Stratus\n",
    "\n",
    "constraints = {\n",
    "    'start_time': 1552608000,\n",
    "    'end_time': 1552694400,\n",
    "    'datastream': 'corkasacrcfrhsrhiM1.a1'\n",
    "}\n",
    "\n",
    "transfer_files(constraints, Stratus)\n",
    "```\n",
    "\n",
    "You can also pass the `--dry-run` flag or set the `dry_run` key in the constraints dictionary to `True` in order to calculate size and volume without actually transfering anything. The `stage_arm_data` tooling tries to be efficient by using the `/lustre/or-hydra/cades-arm/proj-shared/data_transfer` area as a read-only common cache of transfered files. If a file already exists in the transfer area, and md5 check will be conducted and the file will only be conducted if a newer version of that file is available from the archive (usually due to reprocessing efforts to address DQRs). If a file does have to be transferred from the archive, we leverage the Globus backend in order to transfer files in parallel as quickly as possible.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "Create a `delta` column containing the difference between the `top` in this row and the `bottom` of the previous row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cloud_delta(df):\n",
    "    df['delta'] = (df['bottom'].shift(-1) - df['top']).fillna(np.inf)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign successive group ids to the detected cloud layers. If two layers are less than 120m appart (calculated using the `delta` column from the function above), they will be assigned to the same group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_group(df):\n",
    "    group = 0\n",
    "    for index, row in df.iterrows():\n",
    "        if row['delta'] > 120:\n",
    "            group += 1\n",
    "        df.at[index, 'group'] = group\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the nsaarscl1clothC1.c1 file for the given day from the ARM archive and extract the relevant columns:\n",
    "1. `CloudLayerBottomHeightMplZwang` - The bottom height (in meters) of the detected cloud layer calculated using Zwang's Mpl method\n",
    "2. `CloudLayerTopHeightMplZwang` - The top height (in meters) of the detected cloud layer calculated using Zwang's Mpl method\n",
    "3. `base_time` - The timestamp on which the time_offset for each row is based\n",
    "4. `time_offset` - Seconds since base time at which this layer was detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cloud_df(day):\n",
    "    with TemporaryDirectory() as working_dir:\n",
    "        arscl_file = download_arm_files(user, credential, 'nsaarscl1clothC1.c1', day, day, working_dir)[0]\n",
    "        arscl_df = xr.open_dataset(arscl_file).to_dataframe()\n",
    "        cloud_df = arscl_df.loc[:, ['CloudLayerBottomHeightMplZwang','CloudLayerTopHeightMplZwang', 'base_time', 'time_offset']]\n",
    "        del arscl_df\n",
    "        return cloud_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nsaarscl1clothC1.c1 data has 3 dimensions:\n",
    "1. `time_offset` - Seconds since `base_time` at which each detection occured\n",
    "2. `nheights` - The z-order index at which this detection occured (multiple cloud layers detected at the same time)\n",
    "\n",
    "Having the `base_time` and `time_offest` separated into two columns is less intuitive than a single timestamp based column. In addition, the `nheights` level isn't useful for this particular analysis. In order to make out data more intuitive and compact, we drop the `nheights` level and reconsile `base_time` and `time_offset` into a single column. Finally, we reindex based on the new reconsiled `time` column and drop any duplicates created in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_index(cloud_df):\n",
    "    cloud_df.set_index(cloud_df['base_time'] + cloud_df['time_offset'], drop=True, append=True, inplace=True)\n",
    "    cloud_df.index = cloud_df.index.droplevel(['nheights', 'time'])\n",
    "    cloud_df.index.set_names(['numlayers', 'time'], inplace=True)\n",
    "    cloud_df.drop(columns=['base_time', 'time_offset'], inplace=True)\n",
    "    cloud_df = cloud_df[~cloud_df.index.duplicated(keep='first')]\n",
    "    cloud_df = cloud_df.swaplevel(0,1)\n",
    "    cloud_df.sort_index(inplace=True)\n",
    "    return cloud_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nsametC1.b1` datastream contains rain data measured at the same site (nsa) and facility (C1). Download the nsametC1.b1 file for the same day and slice out the `pws_precip_rate_mean_1min` column and append it to the existing data. Since the `nsaarscl1clothC1.c1` data is measured at 10s intervals and the `nsametC1.b1` at 1m, we'll use Pandas' `ffill` method to stretch the `nsametC1.b1` data to fit the `nsaarscl1clothC1.c1` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rain_data(cloud_df, day):\n",
    "    with TemporaryDirectory() as working_dir:\n",
    "        rain_file = download_arm_files(user, credential, 'nsametC1.b1', day, day, working_dir)[0]\n",
    "        rain_df = xr.open_dataset(rain_file).to_dataframe()\n",
    "        rain_df = rain_df['pws_precip_rate_mean_1min'].reindex(cloud_df.index.levels[0], method='ffill')\n",
    "        return cloud_df.join(rain_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rain rates above 1mm/min cause the radar to attenuate too badly for the data to be useful, so we filter out those periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_out_too_rainy(cloud_df):\n",
    "    return cloud_df[cloud_df['pws_precip_rate_mean_1min'] <= 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine consecutive layers separated by less than 120m and drop resulting layers not at least 120m thick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_adjacent_clouds(cloud_df):\n",
    "    cloud_df.columns = ['bottom', 'top', 'rain_rate']\n",
    "    cloud_df = cloud_df.groupby('time').apply(cloud_delta)\n",
    "    cloud_df = cloud_df.groupby('time').apply(rolling_group)\n",
    "    cloud_df = cloud_df.groupby(['time', 'group']).aggregate({'bottom': 'min', 'top': 'max', 'rain_rate': 'mean'})\n",
    "    cloud_df['thickness'] = cloud_df['top'] - cloud_df['bottom']\n",
    "    cloud_df = cloud_df[(cloud_df.thickness > 120) & (cloud_df.bottom > 120)]\n",
    "    return cloud_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign an `int` to the `type` column based on the bottom, top, and thickness of the combined cloud layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_clouds(cloud_df):\n",
    "    low_clouds = (cloud_df.bottom < 3500) & (cloud_df.top < 3500) & (cloud_df.thickness < 3500)\n",
    "    congestus = (cloud_df.bottom < 3500) & (cloud_df.top >= 3500) & (cloud_df.top <= 6500) & (cloud_df.thickness >= 1500)\n",
    "    deep_convection = (cloud_df.bottom < 3500) & (cloud_df.top >= 3500) & (cloud_df.top < 6500) & (cloud_df.thickness >= 1500)\n",
    "    altocumulus = (cloud_df.bottom >= 3500) & (cloud_df.bottom <= 6500) & (cloud_df.top >= 3500) & (cloud_df.top <= 6500) & (cloud_df.thickness < 1500)\n",
    "    altostratus = (cloud_df.bottom >= 3500) & (cloud_df.bottom <= 6500) & (cloud_df.top >= 3500) & (cloud_df.top <= 6500) & (cloud_df.thickness >= 1500)\n",
    "    anvil = (cloud_df.bottom >= 3500) & (cloud_df.bottom <= 6500) & (cloud_df.top > 6500) & (cloud_df.thickness >= 1500)\n",
    "    cirrus = (cloud_df.bottom > 6500) & (cloud_df.top > 6500)\n",
    "\n",
    "    cloud_df.loc[low_clouds, 'type'] = 1\n",
    "    cloud_df.loc[congestus, 'type'] = 2\n",
    "    cloud_df.loc[deep_convection, 'type'] = 3\n",
    "    cloud_df.loc[altocumulus, 'type'] = 4\n",
    "    cloud_df.loc[altostratus, 'type'] = 5\n",
    "    cloud_df.loc[anvil, 'type'] = 6\n",
    "    cloud_df.loc[cirrus, 'type'] = 7\n",
    "    return cloud_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop over all the days in our target window, create a dataframe for the given date and push it through our data pipeline. Display the `head` of the resulting dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>bottom</th>\n",
       "      <th>top</th>\n",
       "      <th>rain_rate</th>\n",
       "      <th>thickness</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th>group</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-03-01 12:42:40</th>\n",
       "      <th>2.0</th>\n",
       "      <td>2042.503052</td>\n",
       "      <td>4315.280762</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2272.777832</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-03-01 12:42:50</th>\n",
       "      <th>2.0</th>\n",
       "      <td>2042.503052</td>\n",
       "      <td>4315.280762</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2272.777832</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-03-01 12:43:00</th>\n",
       "      <th>2.0</th>\n",
       "      <td>2042.503052</td>\n",
       "      <td>4315.280762</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2272.777832</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-03-01 12:43:10</th>\n",
       "      <th>2.0</th>\n",
       "      <td>2042.503052</td>\n",
       "      <td>4315.280762</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2272.777832</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-03-01 12:43:20</th>\n",
       "      <th>2.0</th>\n",
       "      <td>2042.503052</td>\n",
       "      <td>4315.280762</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2272.777832</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                bottom          top  rain_rate    thickness  \\\n",
       "time                group                                                     \n",
       "2010-03-01 12:42:40 2.0    2042.503052  4315.280762        0.0  2272.777832   \n",
       "2010-03-01 12:42:50 2.0    2042.503052  4315.280762        0.0  2272.777832   \n",
       "2010-03-01 12:43:00 2.0    2042.503052  4315.280762        0.0  2272.777832   \n",
       "2010-03-01 12:43:10 2.0    2042.503052  4315.280762        0.0  2272.777832   \n",
       "2010-03-01 12:43:20 2.0    2042.503052  4315.280762        0.0  2272.777832   \n",
       "\n",
       "                           type  \n",
       "time                group        \n",
       "2010-03-01 12:42:40 2.0     3.0  \n",
       "2010-03-01 12:42:50 2.0     3.0  \n",
       "2010-03-01 12:43:00 2.0     3.0  \n",
       "2010-03-01 12:43:10 2.0     3.0  \n",
       "2010-03-01 12:43:20 2.0     3.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>bottom</th>\n",
       "      <th>top</th>\n",
       "      <th>rain_rate</th>\n",
       "      <th>thickness</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th>group</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-03-02 18:24:20</th>\n",
       "      <th>2.0</th>\n",
       "      <td>4533.817383</td>\n",
       "      <td>4796.061035</td>\n",
       "      <td>0.02</td>\n",
       "      <td>262.243652</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-03-02 18:24:30</th>\n",
       "      <th>2.0</th>\n",
       "      <td>4402.695312</td>\n",
       "      <td>4796.061035</td>\n",
       "      <td>0.02</td>\n",
       "      <td>393.365723</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-03-02 18:24:40</th>\n",
       "      <th>2.0</th>\n",
       "      <td>4402.695312</td>\n",
       "      <td>4796.061035</td>\n",
       "      <td>0.02</td>\n",
       "      <td>393.365723</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                bottom          top  rain_rate   thickness  \\\n",
       "time                group                                                    \n",
       "2010-03-02 18:24:20 2.0    4533.817383  4796.061035       0.02  262.243652   \n",
       "2010-03-02 18:24:30 2.0    4402.695312  4796.061035       0.02  393.365723   \n",
       "2010-03-02 18:24:40 2.0    4402.695312  4796.061035       0.02  393.365723   \n",
       "\n",
       "                           type  \n",
       "time                group        \n",
       "2010-03-02 18:24:20 2.0     4.0  \n",
       "2010-03-02 18:24:30 2.0     4.0  \n",
       "2010-03-02 18:24:40 2.0     4.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 46s, sys: 23.8 s, total: 4min 10s\n",
      "Wall time: 2min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "start = '2010-03-01'\n",
    "end = '2010-03-02'\n",
    "for day in pd.date_range(start, end):\n",
    "    cloud_df = (\n",
    "        create_cloud_df(day.date())\n",
    "        .pipe(cleanup_index)\n",
    "        .pipe(add_rain_data, day.date())\n",
    "        .pipe(filter_out_too_rainy)\n",
    "        .pipe(combine_adjacent_clouds)\n",
    "        .pipe(classify_clouds)\n",
    "    )\n",
    "    display(cloud_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://arm-jupyter.ornl.gov:5555\n",
       "  <li><b>Dashboard: </b><a href='http://arm-jupyter.ornl.gov:8787/status' target='_blank'>http://arm-jupyter.ornl.gov:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>6</li>\n",
       "  <li><b>Cores: </b>36</li>\n",
       "  <li><b>Memory: </b>256.02 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://128.219.186.46:5555' processes=6 cores=36>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client, progress\n",
    "\n",
    "client = Client('arm-jupyter.ornl.gov:5555')\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.submit(sum, (1,2)).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d7f4e4be5cc408d83cc471443031635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = '2010-03-01'\n",
    "end = '2010-03-02'\n",
    "\n",
    "jobs = []\n",
    "for day in pd.date_range(start, end):\n",
    "    cloud_df = client.submit(create_cloud_df, day.date())\n",
    "    cloud_df = client.submit(cleanup_index, cloud_df)\n",
    "    cloud_df = client.submit(add_rain_data, cloud_df, day.date())\n",
    "    cloud_df = client.submit(filter_out_too_rainy, cloud_df)\n",
    "    cloud_df = client.submit(combine_adjacent_clouds, cloud_df)\n",
    "    cloud_df = client.submit(classify_clouds, cloud_df)\n",
    "    jobs.append(cloud_df)\n",
    "\n",
    "progress(jobs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
