{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ARM Live credentials here\n",
    "user = \"zprice\"\n",
    "credential = \"64f65bc512a82a27\"\n",
    "\n",
    "# Get your credentials at https://adc.arm.gov/armlive/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import shutil\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions\n",
    "## Download data from the ARM archive\n",
    "Thr ARMLive web service returns a JSON blob with download links for archive files based on the datastream, start, and end dates provided. This function parses the JSON blob and downloads the responsive files into the output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_arm_files(user, token, datastream, start, end, output_directory):\n",
    "    params = {\n",
    "        'user': f'{user}:{token}',\n",
    "       'ds': datastream,\n",
    "       'start': start,\n",
    "       'end': end,\n",
    "       'wt': 'json',\n",
    "    }\n",
    "\n",
    "#     print(params)\n",
    "    response = requests.get('https://adc.arm.gov/armlive/livedata/query', params=params)\n",
    "#     print(response.url)\n",
    "    response = response.json()\n",
    "#     print(response)\n",
    "    downloaded_files = []\n",
    "    for filename in response['files']:\n",
    "        download_url = f'https://adc.arm.gov/armlive/livedata/saveData?user={user}:{token}&file={filename}'\n",
    "        file_path = Path(output_directory, filename)\n",
    "        file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with requests.get(download_url, stream=True) as r:\n",
    "            with open(file_path, 'wb') as f:\n",
    "                shutil.copyfileobj(r.raw, f)\n",
    "        downloaded_files.append(file_path)\n",
    "    return downloaded_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note for users of Stratus:\n",
    "The above function uses the ARMLive web service which is available from anywhere in the world, no setup needed. The drawback to this approach is that the service only provides \"user accessible\" data, which excludes raw and some other types of data. Within the Stratus HPC environment, we provide an HPC module and python library which can be used to stage any file in the archive. You can access these functions by running `module load stage_arm_data` from the terminal or `!module load stage_arm_data` (note the `!`) within a jupyter notebook cell. After loading the module, the `stage_arm_data` command will be added to your `PATH` and the `stage_arm_data` python library will be added to your `PYTHONPATH`. An example invocation from the command line would look like this:\n",
    "`stage_arm_data --to Stratus --datastream corkasacrcfrhsrhiM1.a1 --start 2019-01-01T00:00:00 --end 2019-02-01T00:00:00`\n",
    "This will stage all ARM archive files for the `corkasacrcfrhsrhiM1.a1` from the `2019-01-01T00:00:00` til `2019-02-01T00:00:00` time period to the staging directory at `/lustre/or-hydra/cades-arm/proj-shared/data_transfer/cor/corkasacrcfrhsrhiM1.a1/`.\n",
    "\n",
    "From within a python script, you can also use the provided library to accomplish the same thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stage_arm_data.core import transfer_files\n",
    "from stage_arm_data.endpoints import Stratus\n",
    "\n",
    "constraints = {\n",
    "    'start_time': 1552608000,  # seconds since epoch\n",
    "    'end_time': 1552694400,    # seconds since epoch\n",
    "    'datastream': 'corkasacrcfrhsrhiM1.a1'\n",
    "}\n",
    "\n",
    "transfer_files(constraints, Stratus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also pass the `--dry-run` flag or set the `dry_run` key in the constraints dictionary to `True` in order to calculate size and volume without actually transfering anything. The `stage_arm_data` tooling tries to be efficient by using the `/lustre/or-hydra/cades-arm/proj-shared/data_transfer` area as a read-only common cache of transfered files. If a file already exists in the transfer area, and md5 check will be conducted and the file will only be conducted if a newer version of that file is available from the archive (usually due to reprocessing efforts to address DQRs). If a file does have to be transferred from the archive, we leverage the Globus backend in order to transfer files in parallel as quickly as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `delta` column containing the difference between the `top` in this row and the `bottom` of the previous row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cloud_delta(df):\n",
    "    df['delta'] = (df['bottom'].shift(-1) - df['top']).fillna(np.inf)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign successive group ids to the detected cloud layers. If two layers are less than 120m appart (calculated using the `delta` column from the function above), they will be assigned to the same group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_group(df):\n",
    "    group = 0\n",
    "    for index, row in df.iterrows():\n",
    "        if row['delta'] > 120:\n",
    "            group += 1\n",
    "        df.at[index, 'group'] = group\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the nsaarscl1clothC1.c1 file for the given day from the ARM archive and extract the relevant columns:\n",
    "1. `CloudLayerBottomHeightMplZwang` - The bottom height (in meters) of the detected cloud layer calculated using Zwang's Mpl method\n",
    "2. `CloudLayerTopHeightMplZwang` - The top height (in meters) of the detected cloud layer calculated using Zwang's Mpl method\n",
    "3. `base_time` - The timestamp on which the time_offset for each row is based\n",
    "4. `time_offset` - Seconds since base time at which this layer was detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cloud_df(day):\n",
    "    with TemporaryDirectory() as working_dir:\n",
    "        arscl_file = download_arm_files(user, credential, 'nsaarscl1clothC1.c1', day, day, working_dir)[0]\n",
    "#         arscl_df = xr.open_dataset(arscl_file).to_dataframe()\n",
    "        xr.open_dataset(arscl_file).to_dask_dataframe()\n",
    "        cloud_df = arscl_df.loc[:, ['CloudLayerBottomHeightMplZwang','CloudLayerTopHeightMplZwang', 'base_time', 'time_offset']]\n",
    "        del arscl_df\n",
    "        return cloud_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nsaarscl1clothC1.c1 data has 3 dimensions:\n",
    "1. `time_offset` - Seconds since `base_time` at which each detection occured\n",
    "2. `nheights` - The z-order index at which this detection occured (multiple cloud layers detected at the same time)\n",
    "\n",
    "Having the `base_time` and `time_offest` separated into two columns is less intuitive than a single timestamp based column. In addition, the `nheights` level isn't useful for this particular analysis. In order to make out data more intuitive and compact, we drop the `nheights` level and reconsile `base_time` and `time_offset` into a single column. Finally, we reindex based on the new reconsiled `time` column and drop any duplicates created in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_index(cloud_df):\n",
    "    cloud_df.set_index(cloud_df['base_time'] + cloud_df['time_offset'], drop=True, append=True, inplace=True)\n",
    "    cloud_df.index = cloud_df.index.droplevel(['nheights', 'time'])\n",
    "    cloud_df.index.set_names(['numlayers', 'time'], inplace=True)\n",
    "    cloud_df.drop(columns=['base_time', 'time_offset'], inplace=True)\n",
    "    cloud_df = cloud_df[~cloud_df.index.duplicated(keep='first')]\n",
    "    cloud_df = cloud_df.swaplevel(0,1)\n",
    "    cloud_df.sort_index(inplace=True)\n",
    "    return cloud_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nsametC1.b1` datastream contains rain data measured at the same site (nsa) and facility (C1). Download the nsametC1.b1 file for the same day and slice out the `pws_precip_rate_mean_1min` column and append it to the existing data. Since the `nsaarscl1clothC1.c1` data is measured at 10s intervals and the `nsametC1.b1` at 1m, we'll use Pandas' `ffill` method to stretch the `nsametC1.b1` data to fit the `nsaarscl1clothC1.c1` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rain_data(cloud_df, day):\n",
    "    with TemporaryDirectory() as working_dir:\n",
    "        rain_file = download_arm_files(user, credential, 'nsametC1.b1', day, day, working_dir)[0]\n",
    "#         rain_df = xr.open_dataset(rain_file).to_dataframe()\n",
    "        rain_df = xr.open_dataset(rain_file).to_dask_dataframe()\n",
    "        rain_df = rain_df['pws_precip_rate_mean_1min'].reindex(cloud_df.index.levels[0], method='ffill')\n",
    "        return cloud_df.join(rain_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rain rates above 1mm/min cause the radar to attenuate too badly for the data to be useful, so we filter out those periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_out_too_rainy(cloud_df):\n",
    "    return cloud_df[cloud_df['pws_precip_rate_mean_1min'] <= 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine consecutive layers separated by less than 120m and drop resulting layers not at least 120m thick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_adjacent_clouds(cloud_df):\n",
    "    cloud_df.columns = ['bottom', 'top', 'rain_rate']\n",
    "    cloud_df = cloud_df.groupby('time').apply(cloud_delta)\n",
    "    cloud_df = cloud_df.groupby('time').apply(rolling_group)\n",
    "    cloud_df = cloud_df.groupby(['time', 'group']).aggregate({'bottom': 'min', 'top': 'max', 'rain_rate': 'mean'})\n",
    "    cloud_df['thickness'] = cloud_df['top'] - cloud_df['bottom']\n",
    "    cloud_df = cloud_df[(cloud_df.thickness > 120) & (cloud_df.bottom > 120)]\n",
    "    return cloud_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign an `int` to the `type` column based on the bottom, top, and thickness of the combined cloud layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_clouds(cloud_df):\n",
    "    low_clouds = (cloud_df.bottom < 3500) & (cloud_df.top < 3500) & (cloud_df.thickness < 3500)\n",
    "    congestus = (cloud_df.bottom < 3500) & (cloud_df.top >= 3500) & (cloud_df.top <= 6500) & (cloud_df.thickness >= 1500)\n",
    "    deep_convection = (cloud_df.bottom < 3500) & (cloud_df.top >= 3500) & (cloud_df.top < 6500) & (cloud_df.thickness >= 1500)\n",
    "    altocumulus = (cloud_df.bottom >= 3500) & (cloud_df.bottom <= 6500) & (cloud_df.top >= 3500) & (cloud_df.top <= 6500) & (cloud_df.thickness < 1500)\n",
    "    altostratus = (cloud_df.bottom >= 3500) & (cloud_df.bottom <= 6500) & (cloud_df.top >= 3500) & (cloud_df.top <= 6500) & (cloud_df.thickness >= 1500)\n",
    "    anvil = (cloud_df.bottom >= 3500) & (cloud_df.bottom <= 6500) & (cloud_df.top > 6500) & (cloud_df.thickness >= 1500)\n",
    "    cirrus = (cloud_df.bottom > 6500) & (cloud_df.top > 6500)\n",
    "\n",
    "    cloud_df.loc[low_clouds, 'type'] = 1\n",
    "    cloud_df.loc[congestus, 'type'] = 2\n",
    "    cloud_df.loc[deep_convection, 'type'] = 3\n",
    "    cloud_df.loc[altocumulus, 'type'] = 4\n",
    "    cloud_df.loc[altostratus, 'type'] = 5\n",
    "    cloud_df.loc[anvil, 'type'] = 6\n",
    "    cloud_df.loc[cirrus, 'type'] = 7\n",
    "    return cloud_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop over all the days in our target window, create a dataframe for the given date and push it through our data pipeline. Display the `head` of the resulting dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = '2010-03-01'\n",
    "end = '2010-03-02'\n",
    "for day in pd.date_range(start, end):\n",
    "    cloud_df = (\n",
    "        create_cloud_df(day.date())\n",
    "        .pipe(cleanup_index)\n",
    "        .pipe(add_rain_data, day.date())\n",
    "        .pipe(filter_out_too_rainy)\n",
    "        .pipe(combine_adjacent_clouds)\n",
    "        .pipe(classify_clouds)\n",
    "    )\n",
    "    display(cloud_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_jobqueue import PBSCluster\n",
    "\n",
    "cluster = PBSCluster(\n",
    "    name='dask-worker',\n",
    "    cores=36,\n",
    "    memory='256GB',\n",
    "    processes=6,\n",
    "    local_directory='$localscratch',\n",
    "    queue='batch',\n",
    "    project='arm',\n",
    "    interface='eth0',\n",
    "    walltime='00:10:00',\n",
    "    job_extra=['-W group_list=cades-arm'],\n",
    "    scheduler_port=5555\n",
    ")\n",
    "cluster.scale(3)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, progress\n",
    "\n",
    "start = '2010-03-01'\n",
    "end = '2010-03-02'\n",
    "\n",
    "client = Client(cluster)\n",
    "for day in pd.date_range(start, end):\n",
    "    cloud_df = client.submit(create_cloud_df, day.date())\n",
    "    cloud_df = client.submit(cleanup_index, cloud_df)\n",
    "    cloud_df = client.submit(add_rain_data, cloud_df, day.date())\n",
    "    cloud_df = client.submit(filter_out_too_rainy, cloud_df)\n",
    "    cloud_df = client.submit(combine_adjacent_clouds, cloud_df)\n",
    "    cloud_df = client.submit(classify_clouds, cloud_df)\n",
    "    cloud_df.result().head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dask-env]",
   "language": "python",
   "name": "conda-env-dask-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
