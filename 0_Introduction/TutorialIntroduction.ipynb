{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tutorial: Using ARM compute resources to do CESD science at scale\n",
    "## ARM/ASR PI Meeting, June 10, 2019\n",
    "Jitendra (Jitu) Kumar, ORNL\n",
    "Scott Collis, ANL\n",
    "Bobby Jackson, ANL\n",
    "Zach Price, ORNL\n",
    "\n",
    "**Special shout out to Zach Price (ORNL) for his incredible effort building the backend infrastructure.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Objectives for this tutorial:\n",
    "- Introduction to ARM computational resources available to science users\n",
    "- How to get access to these resources \n",
    "- **Data Movement**: Tools for seamless access to ARM data for your analysis\n",
    "- **HPC 101**: Help you get started on using these high performance computing resources\n",
    "    -- Introduction to Python programming \n",
    "    -- Parallel programming using Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Limited scope:\n",
    "- Introduction to key elements of ARM HPC resources, get in touch with us for topics we have left out.\n",
    "- Due to limited time, we don't expect this to be a comprehensive HPC course. We have can help guide you to additional resources, and/or if there's interest we can potentially plan a more comprehensive tutorial in future.\n",
    "- We won't be running large jobs or analyzing large data sets, but you will have a head start to scale up after today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ARM Next-Generation Computing Facility\n",
    "\n",
    "![ARM Next-Generation Computing Facility](figures/arm-hpc_ARMASR2019_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Stratus** | **Cumulus** | **JupyterLab** \n",
    "------------|-------------|----------------\n",
    "**A small-scale 30 node Cray cluster** | **A mid-range Cray system** | **Jupyter interactive computing environment**\n",
    "**Total cores**: 1,080 cores  | **Total cores**: 4,032 cores |\n",
    "**Memory**: 256 GB per node | **Memory**: 128 GB per node |\n",
    "**Network**: Mellanox Infiniband network  | **Network**: Cray |\n",
    "**Storage**: Tiered storage  architecture with 100 TB Lustre, 50 GB per user NFS HOME, 400 TB NFS, and 1.92 TB Solid State Drive per node. |  **Storage**: 50 GB per user NFS HOME, 2 PB GPFS storage | \n",
    "Operational data and VAP processing, science users | Primarily used for high-end  modeling, and for routine operations of Large-Eddy Simulation (LES) ARM Symbiotic Simulation and Observation (LASSO). | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARM JupyterLab Interactive Environment\n",
    "![ARM JupyterLab Interactive Environment](figures/arm-jupyter_ARMASR2019.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
